---
title: "kddata_randomforest"
output: html_document
---

Random Forest algorithm is a classification algorithm based on ensemble learning. It works by building multiple decision trees at training and the developed decision trees forms the output function.

Installing the necessary packages:

```{r}
library(randomForest)
library(caret)
library(e1071)
library(onehot)
```

Importing the data as a CSV and reformatting:

```{r}
kddata<-read.csv("//Users//samanthawise//Documents//VersionControl//datasciencetoolbox//assignment_1//data//kddcup.data_10_percent") # edit path

kddnames <- read.table("//Users//samanthawise//Documents//VersionControl//datasciencetoolbox//assignment_1//data//kddcup.names",sep=":",skip=1,as.is=T) # edit path

colnames(kddata) <- c(kddnames[,1],"label")

kddata$label <- as.character(kddata$label)
```

Encoding the categorical columns:

```{r}
kddata$protocol_type <- as.integer(factor(kddata$protocol_type))
kddata$service <- as.integer(factor(kddata$service))
kddata$flag <- as.integer(factor(kddata$flag))

## dimensionality reduction

kddata1 <- kddata[,c('duration','src_bytes','dst_bytes','num_compromised','num_root','count','srv_count','dst_host_count','dst_host_srv_count','label')]

# kddata$label <-onehot(kddata, cols = label, stringsAsFactors = TRUE) 
```


Creating the training-testing partitioning of the dataset:

```{r}
inTrain <- createDataPartition(y=kddata$label,p=0.5,list=FALSE)

kddata_normal <- kddata %>%
  filter(label == "normal.")

kddata_smurf <- kddata %>%
  filter(label == "smurf.")

kddata_attack <- kddata %>%
  filter(label != "normal." & label != "smurf.")

set.seed(1)

training_normal_amount <- floor(0.7*nrow(kddata)*0.2)
normal_amount <- sample(nrow(kddata_normal), training_normal_amount) 

training_normal <- kddata_normal[normal_amount,]
testing_normal <- kddata_normal[-normal_amount,]

training_attack_amount <- floor(0.7*nrow(kddata)*0.8)
testing_attack_amount <- floor(0.3*nrow(kddata)*0.8)
smurf_amount_train <- sample(nrow(kddata_smurf), training_attack_amount)
smurf_amount_test <- sample(nrow(kddata_smurf), testing_attack_amount)

training_attack1 <- kddata_smurf[smurf_amount,]
training_attack2 <- kddata_attack
testing_attack1 <- kddata %>%
  filter(label != "normal." & label != "smurf.")
testing_attack2 <- kddata_smurf[smurf_amount,]

training1 <- rbind(training_normal, training_attack1)
testing1 <- rbind(testing_normal, testing_attack1)

training2 <- rbind(training_normal, training_attack2)
testing2 <- rbind(testing_normal, testing_attack2)

training1$label[which(training1$label != 'normal.')] <- "attack."
testing1$label[which(testing1$label != 'normal.')] <- "attack."

training2$label[which(training2$label != 'normal.')] <- "attack."
testing2$label[which(testing2$label != 'normal.')] <- "attack."

training1$label <- factor(training1$label)
testing1$label <- factor(testing1$label)

training2$label <- factor(training2$label)
testing2$label <- factor(testing2$label)


kddata$label[which(kddata$label != 'normal.')] <- "attack."
kddata$label <- factor(kddata$label)

# write_csv(training, "training_kddata")
# write_csv(testing, "testing_kddata")
```

Taking a look at the entire dataset:

```{r}
str(kddata)
```

Training the model:

The training dataset contains 70% of the total datasize where 20% of that data contains "normal." traffic and the rest contain attack "smurf." traffic. The testing dataset contains the rest of the normal traffic and all the other attack types. 

```{r}
dim1 <-nrow(training1)
dim(training1)
t0 <- Sys.time()
output.forest1 <- randomForest(label ~ ., data = training1)
t1 <- Sys.time() - t0
print(paste("Classifier trained in",as.numeric(t1),"seconds", sep = " "))
```

This time the testing dataset (30% of total datasize) is broken down to 20% "normal." traffic data and the rest is "smurf." attack data. The training dataset contains the rest of the "normal." traffic and all the other attack types. 

```{r}
dim2 <-nrow(training2)
dim(training2)
t0 <- Sys.time()
output.forest2 <- randomForest(label ~ ., data = training2)
t1 <- Sys.time() - t0
print(paste("Classifier trained in",as.numeric(t1),"seconds", sep = " "))
```

The training and testing data is the whole dataset randomly split into half.

```{r}
training3 <- kddata[inTrain,]
testing3 <- kddata[-inTrain,]
dim3 <-nrow(training3)
dim(training3)
t0 <- Sys.time()
output.forest3 <- randomForest(label ~ ., data = training3)
t1 <- Sys.time() - t0
print(paste("Classifier trained in",as.numeric(t1),"seconds", sep = " "))
```

Dimensionality reduction (selecting 9 variables):

```{r}
training4 <- kddata1[inTrain,]
testing4 <- kddata1[-inTrain,]
dim4 <-nrow(training4)
dim(training4)
t0 <- Sys.time()
output.forest4 <- randomForest(label ~ ., data = training4)
t1 <- Sys.time() - t0
print(paste("Classifier trained in",as.numeric(t1),"seconds", sep = " "))
```


Printing out and plotting the model:

Smurf training.

```{r}
print(output.forest1)
plot (output.forest1)
```

Smurf testing.

```{r}
print(output.forest2)
plot (output.forest2)
```

50/50 split.

```{r}
print(output.forest3)
plot (output.forest3)
```

Dimensionality Reduction:

```{r}
print(output.forest4)
plot (output.forest4)
```


Predicting on the testing dataset:

Smurf training.

```{r}
t0 <- Sys.time()
pred1 <- predict(output.forest1,testing1)
t1 <- Sys.time() - t0
print(paste("Classifier tested in",as.numeric(t1),"seconds", sep = " "))

matrix <- confusionMatrix(pred1, testing1$label)
matrix
matrix$byClass[7]
```
This model did not predict any of the 'normal.' data to be attacks, however it has significantly misclassified the 'attack.' traffic. 

Smurf testing.

```{r}
t0 <- Sys.time()
pred2 <- predict(output.forest2,testing2)
t1 <- Sys.time() - t0
print(paste("Classifier tested in",as.numeric(t1),"seconds", sep = " "))

matrix <- confusionMatrix(pred2, testing2$label)
matrix
matrix$byClass[7]
```
This model did not predict any of 'smurf.' data to be attacks and significantly misclassified the 'normal.' traffic.


50/50 split.

```{r}
t0 <- Sys.time()
pred3 <- predict(output.forest3,testing3)
t1 <- Sys.time() - t0
print(paste("Classifier tested in",as.numeric(t1),"seconds", sep = " "))

matrix <- confusionMatrix(pred3, testing3$label)
matrix
matrix$byClass[7]
```

Dimensionality reduction: 

```{r}
t0 <- Sys.time()
pred4 <- predict(output.forest4,testing4)
t1 <- Sys.time() - t0
print(paste("Classifier tested in",as.numeric(t1),"seconds", sep = " "))

matrix <- confusionMatrix(pred4, testing4$label)
matrix
matrix$byClass[7]
```


Tabulate Performance results:

```{r}
results <- tibble("Type of Model" = c("50-50", "smurf. prediction", "smurf. training", "Red-10 variables"), "Accuracy" = c(0.9997, 0.0922, 0.1957, 0.9996), "False Positive" = c(6/24319, 6/28115, 0, 13/24319), "False Negative" = c(4/66109, 1, 0, 26/198311), "F1" = c(0.9997151, 0, 0.001275565, 0.9997328), "Time" = c(9.21633486747742 + 2.82073497772217, 6.96629954973857 + 11.1565990447998, 9.2627666314443 + 4.29678893089294, 1.32725168069204 + 4.27911186218262))

results
```

Interestingly, we find that the '50-50' split is the most accurate model, closely followed by the 'Red-10' model. Using one attack type to predict the other class of attacks and visa versa returns a poor accuracy (0.0922 and 0.1957 for 'smurf. prediction' and 'smurf. training' resp.), implying that the attack types have specific characteristics which differentiates themselves from all the other attacks. In real life, this may mean that random forests are not the best model for predicting a new type of attack. These models also took slightly longer to train and test than using the models which contained a mix of 'normal.' and 'attack' type data in the training dataset. 
